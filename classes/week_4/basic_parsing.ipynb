{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/gaofeng/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n",
      "(S\n",
      "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (NP-PRD\n",
      "      (NP (NN chairman))\n",
      "      (PP\n",
      "        (IN of)\n",
      "        (NP\n",
      "          (NP (NNP Elsevier) (NNP N.V.))\n",
      "          (, ,)\n",
      "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')\n",
    "for tree in t:\n",
    "    print(tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP\n",
    "    NP -> Det N | Det N PP | 'I'\n",
    "    VP -> V NP | VP PP\n",
    "    Det -> 'an' | 'my'\n",
    "    N -> 'elephant' | 'pajamas'\n",
    "    V -> 'shot'\n",
    "    P -> 'in'\n",
    "  \"\"\")\n",
    "sent = [\"I\", \"shot\", \"an\", \"elephant\", \"in\", \"my\", \"pajamas\"]\n",
    "parser = nltk.ChartParser(grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an elephant shot an elephant\n",
      "an elephant shot an pajamas\n",
      "an elephant shot my elephant\n",
      "an elephant shot my pajamas\n",
      "an elephant shot an elephant in an elephant\n",
      "an elephant shot an elephant in an pajamas\n",
      "an elephant shot an elephant in my elephant\n",
      "an elephant shot an elephant in my pajamas\n",
      "an elephant shot an elephant in an elephant in an elephant\n",
      "an elephant shot an elephant in an elephant in an pajamas\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.generate import generate\n",
    "for sentence in generate(grammar, n=10):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error parsing the sentence\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP\n",
    "    NP -> Det N | Det N PP | 'I' | DT NN | P\n",
    "    VP -> V NP | VP PP\n",
    "    Det -> 'an' | 'my' |'DT'\n",
    "    N -> 'elephant' | 'pajamas'|'NN'\n",
    "    V -> 'shot'|'VBP'|'VBD'\n",
    "    P -> 'in'|'PRP$'| 'PRP'| 'IN'\n",
    "    \n",
    "\n",
    "  \"\"\")\n",
    "sent = [\"I\", \"shot\", \"an\", \"elephant\", \"in\", \"my\", \"sweatshirt\"]\n",
    "try:\n",
    "    parser = nltk.ChartParser(grammar)\n",
    "    for tree in parser.parse(sent):\n",
    "        print(tree)\n",
    "except:\n",
    "    print(\"error parsing the sentence\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'shot', 'him']\n",
      "[('I', 'PRP'), ('shot', 'VBD'), ('him', 'PRP')]\n",
      "['PRP', 'VBD', 'PRP']\n",
      "(S (NP (P PRP)) (VP (V VBD) (NP (P PRP))))\n",
      "tse\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "input = \"I shot him\"\n",
    "tokens = treebank_tokenizer.tokenize(input)\n",
    "print(tokens)\n",
    "\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)\n",
    "pos_only = [tag[1] for tag in pos_tags]\n",
    "print(pos_only)\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "for tree in parser.parse(pos_only):\n",
    "        print(tree)\n",
    "    \n",
    "print(\"tse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanfordcorenlp\n",
      "  Downloading https://files.pythonhosted.org/packages/35/cb/0a271890bbe3a77fc1aca2bc3a58b14e11799ea77cb5f7d6fb0a8b4c46fa/stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from stanfordcorenlp) (2.21.0)\n",
      "Requirement already satisfied: psutil in /anaconda3/lib/python3.7/site-packages (from stanfordcorenlp) (5.6.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->stanfordcorenlp) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->stanfordcorenlp) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests->stanfordcorenlp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->stanfordcorenlp) (2.8)\n",
      "Installing collected packages: stanfordcorenlp\n",
      "Successfully installed stanfordcorenlp-3.9.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stanfordcorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1b8460ee99ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#nlp = StanfordCoreNLP(r'/products/corenlp/stanford-corenlp-full-2018-02-27')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordCoreNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://localhost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'The ship has sailed.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconstituency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/stanfordcorenlp/corenlp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_host, port, memory, lang, timeout, quiet, logging_level)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_ex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Waiting until the server is available.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The server is available.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "#nlp = StanfordCoreNLP(r'/products/corenlp/stanford-corenlp-full-2018-02-27')\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000)\n",
    "sentence = 'The ship has sailed.'\n",
    "constituency = nlp.parse(sentence)\n",
    "print('Constituency Parsing:', constituency)\n",
    "\n",
    "nlp.close() # Do not forget to close! The backend server will consume a lot memery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(constituency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'constituency' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e7ec276e74b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstituency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconst_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstituency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'constituency' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tree import *\n",
    "Tree.fromstring(constituency).pretty_print()\n",
    "const_tree = Tree.fromstring(constituency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodes(parent):\n",
    "    for node in parent:\n",
    "        if type(node) is Tree:\n",
    "            if node.label() == \"ROOT\":\n",
    "                print(\"======== Sentence =========\")\n",
    "                print(\"Sentence:\", \" \".join(node.leaves()))\n",
    "            else:\n",
    "                print(\"Label:\", node.label())\n",
    "                print(\"Leaves:\", node.leaves())\n",
    "\n",
    "            getNodes(node)\n",
    "        else:\n",
    "            print(\"Word:\", node)\n",
    "getNodes(const_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
